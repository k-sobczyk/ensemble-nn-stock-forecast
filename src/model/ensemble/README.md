# Neural Network Ensemble Combinations

This directory contains the implementation of neural network ensemble combinations for stock price prediction research.

## Research Overview

The research investigates specific neural network combinations using three ensemble methods (Voting, Stacking, Blending) to improve stock price prediction accuracy.

### Tested Combinations

#### Pair Combinations
1. **LSTM + GRU**: Two powerful recurrent networks with different gating mechanisms
2. **LSTM + CNN**: High diversity combination (sequential + local pattern detection)
3. **GRU + Bi-LSTM**: Recurrent pair with bidirectional context understanding
4. **GRU + CNN**: High diversity combination (efficient RNN + feature extraction)
5. **Bi-LSTM + CNN**: High diversity (bidirectional context + local patterns)

#### Triplet Combinations
1. **LSTM + GRU + CNN**: Traditional approach combining two RNNs with CNN
2. **GRU + Bi-LSTM + CNN**: Modern approach with efficient RNN, bidirectional context, and feature extraction

### Ensemble Methods

For each combination, three ensemble strategies are applied:

1. **Voting Ensemble**: Weighted averaging with optimized weights
2. **Stacking Ensemble**: Ridge meta-model with 3-fold cross-validation
3. **Blending Ensemble**: Ridge meta-model with 20% holdout for blending

## Files

### Core Implementation
- `ensemble_base.py`: Base classes and utilities for ensemble methods
- `voting_ensemble.py`: Voting ensemble implementation
- `stacking_ensemble.py`: Stacking ensemble implementation
- `blending_ensemble.py`: Blending ensemble implementation

### Enhanced Runners
- `enhanced_ensemble_runner.py`: Main enhanced runner with comprehensive output
- `neural_network_combinations_runner.py`: Original combinations runner
- `ensemble_runner.py`: Basic ensemble comparison runner
- `run_research_combinations.py`: Research-specific execution script

### Output Structure
- `output/`: Directory containing all results
  - `{combination_name}/`: Individual combination results
    - `{method}/`: Results for each ensemble method
      - `*_detailed_results.json`: Comprehensive results
      - `*_metrics.csv`: Performance metrics
      - `*_predictions.csv`: Actual vs predicted values
    - `*_predictions_comparison.png`: Visualization of predictions
    - `*_metrics_comparison.png`: Metrics comparison chart
  - `ensemble_comprehensive_results.csv`: All results summary
  - `ensemble_performance_heatmap.png`: Performance heatmap
  - `ensemble_performance_distribution.png`: Distribution analysis
  - `top_ensemble_combinations.png`: Best combinations ranking

## Usage

### Quick Demo (15 epochs)
```bash
python run_research_combinations.py --mode demo --epochs 15
```

### Pair Combinations Only
```bash
python run_research_combinations.py --mode pairs --epochs 25
```

### Triplet Combinations Only
```bash
python run_research_combinations.py --mode triplets --epochs 25
```

### Complete Analysis (All Combinations)
```bash
python run_research_combinations.py --mode complete --epochs 30
```

### Custom Dataset
```bash
python run_research_combinations.py --mode complete --epochs 30 --dataset path/to/your/dataset.csv
```

## Output Interpretation

### Metrics
- **RMSE**: Root Mean Square Error (lower is better)
- **MAE**: Mean Absolute Error (lower is better)
- **RÂ²**: R-squared correlation coefficient (higher is better, max 1.0)
- **MAPE**: Mean Absolute Percentage Error (lower is better)
- **SMAPE**: Symmetric Mean Absolute Percentage Error (lower is better)
- **MASE**: Mean Absolute Scaled Error (lower is better)

### Expected Results
Based on ensemble theory, the high-diversity combinations (CNN + RNN) typically perform better than similar architectures combined together.

## Research Questions Addressed

1. **Which neural network combinations perform best for stock prediction?**
2. **How do different ensemble methods compare (Voting vs Stacking vs Blending)?**
3. **Do triplet combinations significantly outperform pair combinations?**
4. **What is the optimal balance between model diversity and complexity?**

## Technical Details

- **Sequence Length**: Automatically optimized based on data characteristics
- **Training/Validation Split**: 80/20 split of training data
- **Test Period**: Data from 2021 onwards (configurable)
- **Early Stopping**: Implemented in individual model training
- **Cross-Validation**: 3-fold CV for stacking ensemble
- **Weight Optimization**: Automated for voting ensemble

## Dependencies

- PyTorch
- scikit-learn
- pandas
- numpy
- matplotlib
- seaborn

## Performance Monitoring

Each run creates comprehensive logs and visualizations to monitor:
- Training progress and convergence
- Individual model contributions
- Ensemble method effectiveness
- Computational efficiency (training times)
- Prediction accuracy across different metrics
