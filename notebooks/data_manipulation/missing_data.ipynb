{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T10:36:01.425324Z",
     "start_time": "2025-02-23T10:36:01.097412Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1d184ce879093af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T10:36:01.465315Z",
     "start_time": "2025-02-23T10:36:01.429642Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/processed/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233980d5ff62be5a",
   "metadata": {},
   "source": [
    "max number of rows that can occur: 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f49108032b134e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T10:53:55.668098Z",
     "start_time": "2025-02-23T10:53:55.652345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tickers: 303\n",
      "\n",
      "\n",
      "['11B' '3RG' 'ABE' 'ABS' 'ACG' 'ACP' 'ACT' 'ADV' 'AGO' 'AGT' 'ALG' 'ALL'\n",
      " 'AMB' 'AMC' 'AML' 'ANR' 'APE' 'APL' 'APN' 'APR' 'APT' 'ARR' 'ART' 'ASB'\n",
      " 'ASE' 'ASM' 'ATC' 'ATD' 'ATG' 'ATP' 'ATR' 'ATT' 'AWM' 'B24' 'BAH' 'BBT'\n",
      " 'BCM' 'BCS' 'BCX' 'BDX' 'BDZ' 'BFT' 'BIO' 'BIP' 'BMC' 'BMX' 'BOW' 'BRA'\n",
      " 'BRG' 'BRS' 'CAR' 'CCC' 'CDL' 'CDR' 'CEZ' 'CFI' 'CIE' 'CIG' 'CLN' 'CMP'\n",
      " 'CMR' 'CNT' 'COG' 'CPL' 'CPR' 'CPS' 'CRI' 'CRJ' 'CRM' 'CTS' 'CTX' 'DAD'\n",
      " 'DAT' 'DBC' 'DCR' 'DEK' 'DEL' 'DIG' 'DNP' 'DOM' 'DPL' 'DTR' 'EAH' 'EAT'\n",
      " 'EDI' 'EEX' 'EFK' 'ELT' 'ELZ' 'EMC' 'ENA' 'ENE' 'ENG' 'ENI' 'ENP' 'ENT'\n",
      " 'ERB' 'ERG' 'ETL' 'EUR' 'FEE' 'FER' 'FRO' 'FSG' 'FTE' 'GIF' 'GIG' 'GOB'\n",
      " 'GOP' 'GPP' 'GRN' 'HDR' 'HEL' 'HLD' 'HRP' 'HRS' 'HUG' 'ICE' 'IFI' 'IFR'\n",
      " 'IMC' 'IMS' 'INK' 'INL' 'IPO' 'IRL' 'ITB' 'IZB' 'IZO' 'IZS' 'JSW' 'JWW'\n",
      " 'KCH' 'KCI' 'KDM' 'KER' 'KGH' 'KGL' 'KGN' 'KMP' 'KOM' 'KPD' 'KPL' 'KRK'\n",
      " 'KTY' 'KVT' 'LAB' 'LBT' 'LBW' 'LEN' 'LES' 'LPP' 'LRK' 'LRQ' 'LSI' 'LTX'\n",
      " 'LWB' 'MAB' 'MAK' 'MAN' 'MBR' 'MCR' 'MDG' 'MDI' 'MEG' 'MEX' 'MFO' 'MGT'\n",
      " 'MIR' 'MLK' 'MLS' 'MNC' 'MOC' 'MOJ' 'MON' 'MRB' 'MRC' 'MSP' 'MSW' 'MSZ'\n",
      " 'MXC' 'MZA' 'NEU' 'NNG' 'NTT' 'NTU' 'NVA' 'NVT' 'NWG' 'NXG' 'OBL' 'ODL'\n",
      " 'OEX' 'OND' 'OPL' 'OPM' 'OPN' 'OTM' 'OTS' 'PAT' 'PBG' 'PBX' 'PCE' 'PCF'\n",
      " 'PCG' 'PCR' 'PCX' 'PEN' 'PEP' 'PGE' 'PGM' 'PHR' 'PJP' 'PKN' 'PKP' 'PLW'\n",
      " 'PMA' 'PMP' 'PPS' 'PRM' 'PRT' 'PTG' 'PTH' 'PUR' 'PWX' 'PXM' 'QNT' 'RAF'\n",
      " 'RBW' 'RDN' 'REG' 'RES' 'RFK' 'RLP' 'RMK' 'RPC' 'RVU' 'RWL' 'SEK' 'SEL'\n",
      " 'SES' 'SFG' 'SFS' 'SGN' 'SHO' 'SIM' 'SKA' 'SLV' 'SLZ' 'SNK' 'SNT' 'SNW'\n",
      " 'SNX' 'SOL' 'SON' 'SPH' 'SPR' 'STF' 'STH' 'STP' 'STS' 'STX' 'SUW' 'SVRS'\n",
      " 'SWG' 'TAR' 'TBL' 'TEN' 'TIM' 'TLX' 'TMR' 'TOA' 'TOR' 'TPE' 'TRK' 'TRN'\n",
      " 'TRR' 'TSG' 'TXM' 'ULG' 'ULM' 'UNI' 'UNT' 'VGO' 'VOX' 'VRC' 'VRG' 'VTL'\n",
      " 'VVD' 'WAS' 'WLT' 'WOJ' 'WPL' 'WTN' 'WWL' 'XTP' 'ZAP' 'ZEP' 'ZMT' 'ZRE'\n",
      " 'ZUE' 'ZUK' 'ZWC']\n"
     ]
    }
   ],
   "source": [
    "num_unique_tickers = df['ticker'].nunique()\n",
    "print(f\"Number of unique tickers: {num_unique_tickers}\")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "unique_tickers = df['ticker'].unique()\n",
    "print(unique_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67abaf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics:\n",
      "Maximum entries for a ticker: 99\n",
      "Minimum entries for a ticker: 1\n",
      "Average entries per ticker: 51.14\n",
      "Median entries per ticker: 50.00\n"
     ]
    }
   ],
   "source": [
    "ticker_counts = df['ticker'].value_counts()\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(f\"Maximum entries for a ticker: {ticker_counts.max()}\")\n",
    "print(f\"Minimum entries for a ticker: {ticker_counts.min()}\")\n",
    "print(f\"Average entries per ticker: {ticker_counts.mean():.2f}\")\n",
    "print(f\"Median entries per ticker: {ticker_counts.median():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8cc6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop tickers that are insufficient\n",
    "\n",
    "tickers_to_drop = ['AGT', 'ANR', 'ASB', 'BBT', 'BCS', 'BCX', 'CRI', 'CRJ', 'CTS', 'CTX', 'DAD',\n",
    "                  'DNP', 'GIF', 'GOP', 'GPP', 'HLD', 'HUG', 'ICE', 'IMC', 'KDM', 'KER', 'MLK',\n",
    "                  'MLS', 'MOC', 'NNG', 'NTU', 'OND', 'PCF', 'PTG', 'PUR', 'SFG', 'SHO', 'SIM',\n",
    "                  'SLV', 'SLZ', 'SPH', 'SPR', 'STH', 'SVRS', 'TEN', 'TMR', 'TXM', 'VRC']\n",
    "\n",
    "\n",
    "df = df[~df['ticker'].isin(tickers_to_drop)]\n",
    "\n",
    "\n",
    "df['end_of_period'] = df['end_of_period'].astype('datetime64[ns]')\n",
    "cutoff_date = pd.to_datetime('2022-07-01')\n",
    "df = df[df['end_of_period'] <= cutoff_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8d01791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_quarters(df):\n",
    "   new_rows = []\n",
    "\n",
    "   for ticker in df['ticker'].unique():\n",
    "       ticker_data = df[df['ticker'] == ticker]\n",
    "\n",
    "       ticker_data = ticker_data.sort_values('end_of_period')\n",
    "\n",
    "       first_date = ticker_data['end_of_period'].min()\n",
    "       last_date = ticker_data['end_of_period'].max()\n",
    "\n",
    "       day_of_month = ticker_data['end_of_period'].dt.day.mode()[0]\n",
    "\n",
    "       all_quarters = []\n",
    "       current_date = first_date\n",
    "\n",
    "       while current_date <= last_date:\n",
    "           all_quarters.append(current_date)\n",
    "           year = current_date.year + (current_date.month + 3) // 12\n",
    "           month = (current_date.month + 3 - 1) % 12 + 1\n",
    "           current_date = pd.Timestamp(year=year, month=month, day=day_of_month)\n",
    "\n",
    "       existing_dates = set(ticker_data['end_of_period'])\n",
    "       all_quarters_set = set(all_quarters)\n",
    "       missing_dates = all_quarters_set - existing_dates\n",
    "\n",
    "       for missing_date in missing_dates:\n",
    "           new_row = {'ticker': ticker, 'end_of_period': missing_date}\n",
    "           new_rows.append(new_row)\n",
    "\n",
    "   if new_rows:\n",
    "       missing_df = pd.DataFrame(new_rows)\n",
    "\n",
    "       for col in df.columns:\n",
    "           if col not in ['ticker', 'end_of_period']:\n",
    "               missing_df[col] = np.nan\n",
    "\n",
    "       result_df = pd.concat([df, missing_df], ignore_index=True)\n",
    "       result_df = result_df.sort_values(['ticker', 'end_of_period'])\n",
    "\n",
    "       return result_df\n",
    "\n",
    "   return df\n",
    "\n",
    "\n",
    "df = fill_missing_quarters(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "854f7295",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stooq = pd.read_csv('../../data/processed/stooq_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2c28933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1594620 entries, 0 to 1594619\n",
      "Data columns (total 11 columns):\n",
      " #   Column   Non-Null Count    Dtype  \n",
      "---  ------   --------------    -----  \n",
      " 0   TICKER   1594620 non-null  object \n",
      " 1   PER      1594620 non-null  object \n",
      " 2   DATE     1594620 non-null  object \n",
      " 3   TIME     1594620 non-null  int64  \n",
      " 4   OPEN     1594620 non-null  float64\n",
      " 5   HIGH     1594620 non-null  float64\n",
      " 6   LOW      1594620 non-null  float64\n",
      " 7   CLOSE    1594620 non-null  float64\n",
      " 8   VOL      1594620 non-null  float64\n",
      " 9   OPENINT  1594620 non-null  int64  \n",
      " 10  target   1594620 non-null  float64\n",
      "dtypes: float64(6), int64(2), object(3)\n",
      "memory usage: 133.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_stooq.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9394bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def update_null_targets(df, df_stooq, tolerance_days=7):\n",
    "    result_df = df.copy()\n",
    "\n",
    "    if result_df['end_of_period'].dtype != 'datetime64[ns]':\n",
    "        result_df['end_of_period'] = pd.to_datetime(result_df['end_of_period'])\n",
    "\n",
    "    df_stooq_prep = df_stooq.copy()\n",
    "    df_stooq_prep['DATE'] = pd.to_datetime(df_stooq_prep['DATE'])\n",
    "    df_stooq_prep = df_stooq_prep.rename(columns={\n",
    "        'TICKER': 'ticker',\n",
    "        'DATE': 'end_of_period',\n",
    "    })\n",
    "\n",
    "    null_target_rows = result_df[result_df['target'].isna()].copy()\n",
    "\n",
    "    if len(null_target_rows) == 0:\n",
    "        return result_df\n",
    "\n",
    "    merged_groups = []\n",
    "\n",
    "    for ticker, group in null_target_rows.groupby('ticker'):\n",
    "        group = group.sort_values('end_of_period')\n",
    "\n",
    "        stooq_group = df_stooq_prep[df_stooq_prep['ticker'] == ticker].sort_values('end_of_period')\n",
    "\n",
    "        if stooq_group.empty:\n",
    "            merged_groups.append(group)\n",
    "            continue\n",
    "\n",
    "        stooq_group = stooq_group[['ticker', 'end_of_period', 'target']]\n",
    "\n",
    "        try:\n",
    "            merged = pd.merge_asof(\n",
    "                group,\n",
    "                stooq_group,\n",
    "                on='end_of_period',\n",
    "                by='ticker',\n",
    "                direction='nearest',\n",
    "                tolerance=pd.Timedelta(days=tolerance_days),\n",
    "                suffixes=('', '_stooq')\n",
    "            )\n",
    "\n",
    "            if 'target_stooq' in merged.columns:\n",
    "                merged['target'] = merged['target_stooq'].combine_first(merged['target'])\n",
    "                merged = merged.drop('target_stooq', axis=1)\n",
    "\n",
    "            merged_groups.append(merged)\n",
    "\n",
    "        except Exception as e:\n",
    "            merged_groups.append(group)\n",
    "\n",
    "    updated_rows = pd.concat(merged_groups, ignore_index=True) if merged_groups else pd.DataFrame()\n",
    "\n",
    "    if not updated_rows.empty:\n",
    "        non_null_rows = result_df[~result_df['target'].isna()].copy()\n",
    "        result_df = pd.concat([non_null_rows, updated_rows], ignore_index=True)\n",
    "        result_df = result_df.sort_values(['ticker', 'end_of_period'])\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34938d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_null_targets(df, df_stooq, tolerance_days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77d848cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (15528, 17)\n",
      "[IterativeImputer] Ending imputation round 1/10, elapsed time 497.93\n",
      "[IterativeImputer] Change: 4750006.83743915, scaled tolerance: 1396695.0 \n",
      "[IterativeImputer] Ending imputation round 2/10, elapsed time 1040.11\n",
      "[IterativeImputer] Change: 2148887.1051999996, scaled tolerance: 1396695.0 \n",
      "[IterativeImputer] Ending imputation round 3/10, elapsed time 1509.08\n",
      "[IterativeImputer] Change: 1685603.5993400002, scaled tolerance: 1396695.0 \n",
      "[IterativeImputer] Ending imputation round 4/10, elapsed time 1982.00\n",
      "[IterativeImputer] Change: 1638688.275, scaled tolerance: 1396695.0 \n",
      "[IterativeImputer] Ending imputation round 5/10, elapsed time 2482.08\n",
      "[IterativeImputer] Change: 1926642.8484300002, scaled tolerance: 1396695.0 \n",
      "[IterativeImputer] Ending imputation round 6/10, elapsed time 3233.37\n",
      "[IterativeImputer] Change: 1428875.15543, scaled tolerance: 1396695.0 \n",
      "[IterativeImputer] Ending imputation round 7/10, elapsed time 3928.32\n",
      "[IterativeImputer] Change: 2266397.4999, scaled tolerance: 1396695.0 \n",
      "[IterativeImputer] Ending imputation round 8/10, elapsed time 4400.71\n",
      "[IterativeImputer] Change: 2712450.7794399997, scaled tolerance: 1396695.0 \n",
      "[IterativeImputer] Ending imputation round 9/10, elapsed time 4881.80\n",
      "[IterativeImputer] Change: 2528264.9138399996, scaled tolerance: 1396695.0 \n",
      "[IterativeImputer] Ending imputation round 10/10, elapsed time 5398.24\n",
      "[IterativeImputer] Change: 2649074.3499399996, scaled tolerance: 1396695.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ksobc\\PycharmProjects\\ensemble-nn-stock-forecast\\.venv\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before imputation:\n",
      "total_assets                              412\n",
      "non_current_assets                        412\n",
      "current_assets                            412\n",
      "property_plant_equipment                  412\n",
      "intangible_assets                         412\n",
      "inventories                               412\n",
      "trade_receivables                         412\n",
      "cash_and_cash_equivalents                 412\n",
      "equity_shareholders_of_the_parent         412\n",
      "share_capital                             412\n",
      "retained_earning_accumulated_losses       412\n",
      "non_current_liabilities                   412\n",
      "current_liabilities                       412\n",
      "non_current_loans_and_borrowings          412\n",
      "financial_liabilities_loans_borrowings    412\n",
      "total_shares                              412\n",
      "target                                     29\n",
      "dtype: int64\n",
      "\n",
      "Missing values after imputation:\n",
      "total_assets                              0\n",
      "non_current_assets                        0\n",
      "current_assets                            0\n",
      "property_plant_equipment                  0\n",
      "intangible_assets                         0\n",
      "inventories                               0\n",
      "trade_receivables                         0\n",
      "cash_and_cash_equivalents                 0\n",
      "equity_shareholders_of_the_parent         0\n",
      "share_capital                             0\n",
      "retained_earning_accumulated_losses       0\n",
      "non_current_liabilities                   0\n",
      "current_liabilities                       0\n",
      "non_current_loans_and_borrowings          0\n",
      "financial_liabilities_loans_borrowings    0\n",
      "total_shares                              0\n",
      "target                                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "datetime_cols = ['end_of_period']\n",
    "object_cols = ['file_name', 'ticker', 'sector']\n",
    "numeric_cols = [col for col in df.columns if col not in datetime_cols + object_cols]\n",
    "numeric_data = df[numeric_cols].copy()\n",
    "\n",
    "# Initialize the MICE imputer\n",
    "# Using RandomForestRegressor as the estimator often gives good results\n",
    "mice_imputer = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    max_iter=10,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "imputed_numeric_data = mice_imputer.fit_transform(numeric_data)\n",
    "imputed_df = pd.DataFrame(imputed_numeric_data, columns=numeric_cols)\n",
    "\n",
    "# Add back the non-numeric columns\n",
    "for col in datetime_cols + object_cols:\n",
    "    imputed_df[col] = df[col].values\n",
    "\n",
    "# Verify the imputation results\n",
    "print(\"Missing values before imputation:\")\n",
    "print(df[numeric_cols].isna().sum())\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(imputed_df[numeric_cols].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c50b0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df.to_csv('../../data/filled_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
